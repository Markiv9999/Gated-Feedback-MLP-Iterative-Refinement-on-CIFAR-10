import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision.datasets import CIFAR10from torchvision import transformsfrom torch.utils.data import DataLoaderimport numpy as np# -------------------------------# --- Data Loading & Normalization# -------------------------------transform_train = transforms.Compose([    transforms.RandomHorizontalFlip(),    transforms.RandomCrop(32, padding=4),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])transform_test = transforms.Compose([    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])train_ds = CIFAR10(root='./data', train=True, download=True, transform=transform_train)test_ds = CIFAR10(root='./data', train=False, download=True, transform=transform_test)batch_size = 128train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)device = torch.device("cuda" if torch.cuda.is_available() else "cpu")# -------------------------------# --- Gated Feedback MLP# -------------------------------class GatedFeedbackNet(nn.Module):    def __init__(self, input_dim=32*32*3, state_dim=256, n_blocks=3, feedback_scale=0.3):        super().__init__()        self.input_proj = nn.Linear(input_dim, state_dim)        self.blocks = nn.ModuleList([            nn.Sequential(                nn.Linear(state_dim, state_dim),                nn.ReLU(),                nn.LayerNorm(state_dim)            ) for _ in range(n_blocks)        ])        self.readout = nn.Linear(state_dim, 10)  # CIFAR-10 has 10 classes        self.fb_to_input = nn.Linear(state_dim, input_dim)        self.gate_layer = nn.Linear(state_dim, input_dim)        self.feedback_scale = feedback_scale    def forward_once(self, x):        h = F.relu(self.input_proj(x))        for b in self.blocks:            delta = b(h)            h = h + delta        logits = self.readout(h)        return logits, h    def forward(self, x):        logits, _ = self.forward_once(x)        return logits    def infer_loop(self, x0, y_true=None, max_iters=10  , tol=1e-10, verbose=False, return_best=False):        x = x0        prev_logits = None        logits_list = []        best_acc = -1.0        best_logits = None        best_iter_idx = -1        for t in range(max_iters):            logits, h = self.forward_once(x)            logits_list.append(logits.detach().cpu().numpy())            if return_best and y_true is not None:                preds = logits.argmax(dim=1)                acc = (preds == y_true).float().mean().item()                if acc > best_acc:                    best_acc = acc                    best_logits = logits.detach()                    best_iter_idx = t            gate = torch.sigmoid(self.gate_layer(h))            fb = torch.tanh(self.fb_to_input(h)) * gate * self.feedback_scale            x = x + fb            if prev_logits is not None:                if torch.max(torch.abs(logits - prev_logits)) < tol:                    if verbose:                        print(f"Converged at iter {t}")                    break            prev_logits = logits        if return_best and best_logits is not None:            return best_logits, logits_list, best_iter_idx        else:            return logits, logits_list, None# -------------------------------# --- Training & Evaluation# -------------------------------loss_fn = nn.CrossEntropyLoss()def train_epoch(model, optimizer, max_iters=5, tol=1e-10):    model.train()    total_loss = 0.0    total_batches = len(train_loader)    for batch_idx, (xb, yb) in enumerate(train_loader):        xb = xb.view(xb.size(0), -1).to(device)        yb = yb.to(device)        optimizer.zero_grad()        x = xb        losses = []        prev_logits = None        for _ in range(max_iters):            logits, h = model.forward_once(x)            loss = loss_fn(logits, yb)            losses.append(loss)            gate = torch.sigmoid(model.gate_layer(h))            fb = torch.tanh(model.fb_to_input(h)) * gate * model.feedback_scale            x = x + fb            if prev_logits is not None and torch.max(torch.abs(logits - prev_logits)) < tol:                break            prev_logits = logits        total_loss_batch = torch.stack(losses).mean()        total_loss_batch.backward()        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)        optimizer.step()        total_loss += total_loss_batch.item() * xb.size(0)        # --- Print epoch progress ---        if batch_idx % 20 == 0 or batch_idx == total_batches - 1:            percent = (batch_idx + 1) / total_batches * 100            print(f"Epoch progress: {percent:.1f}% ({batch_idx+1}/{total_batches} batches)")    return total_loss / len(train_ds)def eval_single_step_acc(model):    model.eval()    total_correct = 0    total_samples = 0    with torch.no_grad():        for xb, yb in test_loader:            xb = xb.view(xb.size(0), -1).to(device)            yb = yb.to(device)            logits = model(xb)            preds = logits.argmax(dim=1)            total_correct += (preds == yb).sum().item()            total_samples += yb.size(0)    return total_correct / total_samplesdef eval_iterative_best_acc(model, max_iters=50, tol=1e-10):    model.eval()    total_correct = 0    total_samples = 0    best_iters = []    with torch.no_grad():        for xb, yb in test_loader:            xb = xb.view(xb.size(0), -1).to(device)            yb = yb.to(device)            best_logits, _, best_iter = model.infer_loop(xb, y_true=yb, max_iters=max_iters, tol=tol, return_best=True)            preds = best_logits.argmax(dim=1)            total_correct += (preds == yb).sum().item()            total_samples += yb.size(0)            best_iters.append(best_iter)    overall_acc = total_correct / total_samples    avg_best_iter = int(np.round(np.mean(best_iters)))    return overall_acc, avg_best_iter# -------------------------------# --- Training Loop# -------------------------------model = GatedFeedbackNet().to(device)optimizer = optim.Adam(model.parameters(), lr=1e-3)epochs = 10for epoch in range(1, epochs+1):    loss = train_epoch(model, optimizer)    single_acc = eval_single_step_acc(model)    iter_best_acc, best_iter = eval_iterative_best_acc(model)    print(f"Epoch {epoch}/{epochs} | Loss: {loss:.4f} | Single-step Acc: {single_acc:.4f} | Iterative Best Acc: {iter_best_acc:.4f} at iter {best_iter}")print("Training complete.")