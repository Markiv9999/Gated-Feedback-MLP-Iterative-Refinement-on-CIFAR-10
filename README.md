ğŸ” Gated Feedback MLP

An experimental Multi-Layer Perceptron (MLP) architecture with gated feedback connections that iteratively refine representations. Inspired by ideas from recurrent networks, feedback CNNs, and deep equilibrium models, this project explores whether simple feedback loops in MLPs can improve performance on standard benchmarks.

âœ¨ Features

ğŸ”¹ Basic MLP backbone with configurable number of layers.

ğŸ”¹ Gated feedback mechanism: hidden states are fed back into the model, allowing iterative refinement.

ğŸ”¹ Leaky ReLU activations for improved stability over vanilla ReLU.

ğŸ”¹ PyTorch implementation, modular and easy to extend.

ğŸ”¹ Experiments on:

CIFAR-10 (image classification)

MNIST (digit recognition)

Two Moons (toy 2D dataset)

ğŸ“Š Results Summary
Dataset	Single-Pass Accuracy	Iterative Best Accuracy	Notes
CIFAR-10	~46% (2-layer MLP)	~56% (after 4 iterations)	Largest gains seen
MNIST	Moderate	Small improvements	Converges quickly
Two Moons	Baseline separation	Improved cluster boundaries	More defined decision boundary
